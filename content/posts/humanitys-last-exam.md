---
title: "ðŸ”— Humanityâ€™s Last Exam"
slug: "humanitys-last-exam"
date: "2025-01-26"
description: "Humanityâ€™s Last Exam â€œBenchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over..."
tags: ["ai", "link-log", "llm"]
draft: false
---

[Humanityâ€™s Last Exam](https://agi.safe.ai/)

> Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam, a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. The dataset consists of 3,000challenging questions across over a hundred subjects. We publicly release these questions, while maintaining a private test set of held out questions to assess model overfitting.

The sample questions are fun to go through, as a way of understanding the level of expertise these models are going to end up at, eventually. _Eventually_  is the keyword there â€” even the best frontier models do very poorly on this benchmark right now.

Via: [Installer newsletter by The Verve](https://www.theverge.com/pages/installer-newsletter-sign-up)
