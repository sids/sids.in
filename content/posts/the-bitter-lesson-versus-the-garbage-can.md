---
title: "ðŸ”— The Bitter Lesson versus The Garbage Can"
slug: "the-bitter-lesson-versus-the-garbage-can"
date: "2025-08-02"
description: "The Bitter Lesson versus The Garbage Can by Ethan Mollick A thought-provoking article that, on the surface, explores which modality of AI agent deployment is more likely to succeed in a large organisation â€” agents carefully designed around organisational processes, or general-purpose agents trained to seek successful outcomes (RL, for example). But dig a little deeper, and it raises a more fundamental question: what shape will successful AI-powered products take?"
tags: ["ai", "link-log"]
draft: false
---

[The Bitter Lesson versus The Garbage Can](https://www.oneusefulthing.org/p/the-bitter-lesson-versus-the-garbage) by Ethan Mollick

A thought-provoking article that, on the surface, explores which modality of AI agent deployment is more likely to succeed in a large organisation â€” agents carefully designed around organisational processes, or general-purpose agents trained to seek successful outcomes (RL, for example).

But dig a little deeper, and it raises a more fundamental question: what shape will successful AI-powered products take?

Ethan Mollick:

> For many people, this may not be a surprise. One thing you learn studying (or working in) organizations is that they are all actually a bit of a mess. In fact, one classic organizational theory is actually called the Garbage Can Model. This views organizations as chaotic â€œgarbage cansâ€ where problems, solutions, and decision-makers are dumped in together, and decisions often happen when these elements collide randomly, rather than through a fully rational process.

> Computer scientist Richard Sutton introduced the concept of the Bitter Lesson in an [influential 2019 essay](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf "The Bitter Lesson by Rich Sutton") where he pointed out a pattern in AI research. Time and again, AI researchers trying to solve a difficult problem, like beating humans in chess, turned to elegant solutions, studying opening moves, positional evaluations, tactical patterns, and endgame databases. Programmers encoded centuries of chess wisdom in hand-crafted software: control the center, develop pieces early, king safety matters, passed pawns are valuable, and so on. Deep Blue, the first chess computer to beat the worldâ€™s best human, used some chess knowledge, but combined that with the brute force of being able to search 200 million positions a second. In 2017, Google released AlphaZero, which could beat humans not just in chess but also in shogi and go, and it did it with no prior knowledge of these games at all. Instead, the AI model trained against itself, playing the games until it learned them. All of the elegant knowledge of chess was irrelevant, pure brute force computing combined with generalized approaches to machine learning, was enough to beat them. And that is the Bitter Lesson â€” encoding human understanding into an AI tends to be worse than just letting the AI figure out how to solve the problem, and adding enough computing power until it can do it better than any human.
